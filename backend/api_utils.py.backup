"""
API utility functions for vision and chat models

FIXED VERSION:
- Gemini model: gemini-2.0-flash (not 2.5)
- Groq Vision as primary
- Rate limit handling for Gemini
- GPT-4o Vision removed (refuses medical images)
- YOLOv8 integration for accurate bounding box detection
"""
import os
import base64
import asyncio
import json
from io import BytesIO
from typing import Dict, Tuple, Optional, List
from openai import OpenAI
from groq import Groq
from google import generativeai as genai
from PIL import Image
import numpy as np
from ultralytics import YOLO


# Initialize clients
def init_clients():
    """Initialize API clients with keys from environment"""
    openai_client = OpenAI(api_key=os.getenv("OPEN_AI_API_KEY"))
    groq_client = Groq(api_key=os.getenv("GROQ_AI_API_KEY"))
    genai.configure(api_key=os.getenv("GOOGLE_AI_API_KEY"))

    return openai_client, groq_client


# ============ YOLO MODEL INITIALIZATION ============
# Global YOLO model instance (lazy loaded)
_yolo_model = None

# Primary model: DENTEX with Impacted class (for wisdom teeth)
YOLO_IMPACTED_MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "dental_impacted.pt")

# Fallback model: General dental model
YOLO_DENTAL_MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "dental_yolo.pt")

# Last resort: Base YOLOv8n
YOLO_FALLBACK_PATH = "yolov8n.pt"

def get_yolo_model():
    """
    Get or initialize YOLO model (lazy loading)
    Priority: Impacted model > General dental > Base YOLOv8n
    """
    global _yolo_model

    if _yolo_model is None:
        try:
            # Try Impacted-specific model first (best for wisdom teeth)
            if os.path.exists(YOLO_IMPACTED_MODEL_PATH):
                print(f"âœ… Loading DENTEX model with Impacted class from {YOLO_IMPACTED_MODEL_PATH}")
                _yolo_model = YOLO(YOLO_IMPACTED_MODEL_PATH)
                print(f"   Model classes: {list(_yolo_model.names.values())}")

            # Fallback to general dental model
            elif os.path.exists(YOLO_DENTAL_MODEL_PATH):
                print(f"âœ… Loading general dental YOLO model from {YOLO_DENTAL_MODEL_PATH}")
                _yolo_model = YOLO(YOLO_DENTAL_MODEL_PATH)
                print(f"   âš ï¸ This model does NOT have 'Impacted' class")

            # Last resort: base model
            else:
                print(f"âš ï¸ No dental models found")
                print(f"ðŸ“¥ Using base YOLOv8n model (NOT trained on dental X-rays)")
                _yolo_model = YOLO(YOLO_FALLBACK_PATH)

        except Exception as e:
            print(f"âŒ Error loading YOLO model: {str(e)}")
            print(f"ðŸ“¥ Downloading base YOLOv8n model...")
            _yolo_model = YOLO(YOLO_FALLBACK_PATH)

    return _yolo_model


def detect_teeth_yolo(image: Image.Image, conf_threshold: float = 0.25) -> Dict:
    """
    Detect teeth in dental X-ray using YOLOv8 model

    Args:
        image: PIL Image of dental X-ray
        conf_threshold: Confidence threshold for detections (default: 0.25)

    Returns:
        Dict with detected teeth information in the format:
        {
            "success": bool,
            "model": str,
            "teeth_found": [
                {
                    "position": str (e.g., "lower-right"),
                    "bbox": [x_min, y_min, x_max, y_max] (normalized 0-1),
                    "confidence": float,
                    "class_name": str (e.g., "Impacted", "Caries"),
                    "description": str
                }
            ],
            "summary": str
        }
    """
    try:
        model = get_yolo_model()

        # Convert PIL Image to numpy array for YOLO
        img_array = np.array(image)

        # Run inference
        results = model(img_array, conf=conf_threshold, verbose=False)

        # Get image dimensions for normalization
        img_height, img_width = img_array.shape[:2]

        teeth_found = []

        # Process detections
        for result in results:
            boxes = result.boxes

            if boxes is None or len(boxes) == 0:
                continue

            for box in boxes:
                # Get box coordinates (xyxy format)
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()

                # Normalize coordinates to 0-1 range
                bbox_normalized = [
                    float(x1 / img_width),
                    float(y1 / img_height),
                    float(x2 / img_width),
                    float(y2 / img_height)
                ]

                # Get confidence and class
                confidence = float(box.conf[0].cpu().numpy())
                class_id = int(box.cls[0].cpu().numpy())
                class_name = model.names[class_id] if hasattr(model, 'names') else f"class_{class_id}"

                # Determine position based on bounding box location
                center_x = (bbox_normalized[0] + bbox_normalized[2]) / 2
                center_y = (bbox_normalized[1] + bbox_normalized[3]) / 2

                if center_y < 0.5:  # Upper half
                    position = "upper-left" if center_x < 0.5 else "upper-right"
                else:  # Lower half
                    position = "lower-left" if center_x < 0.5 else "lower-right"

                # Create description
                description = f"{class_name} (confidence: {confidence:.2f})"

                teeth_found.append({
                    "position": position,
                    "bbox": bbox_normalized,
                    "confidence": confidence,
                    "class_name": class_name,
                    "description": description
                })

        # Sort by confidence
        teeth_found.sort(key=lambda x: x['confidence'], reverse=True)

        # Generate summary
        if len(teeth_found) > 0:
            summary = f"Detected {len(teeth_found)} teeth/dental features using YOLO"
        else:
            summary = "No teeth detected in the X-ray image"

        print(f"  ðŸ¦· YOLO detected {len(teeth_found)} teeth/features")

        return {
            "success": True,
            "model": "YOLOv8 Dental Detection",
            "teeth_found": teeth_found,
            "summary": summary
        }

    except Exception as e:
        print(f"âŒ ERROR in detect_teeth_yolo: {str(e)}")
        import traceback
        traceback.print_exc()

        return {
            "success": False,
            "model": "YOLOv8 Dental Detection",
            "teeth_found": [],
            "summary": f"Error during YOLO detection: {str(e)}"
        }


def encode_image_to_base64(image: Image.Image) -> str:
    """Convert PIL Image to base64 string"""
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')


# ============ CONTEXT-AWARE CHAT FUNCTIONS (TEXT ONLY) ============

async def chat_with_context_async(
    messages: list,
    model_name: str,
    openai_client: OpenAI = None,
    groq_client: Groq = None
) -> Dict:
    """
    Chat with conversation context for any model
    """
    try:
        loop = asyncio.get_event_loop()

        # Clean messages - remove image fields for text-only models
        clean_messages = []
        for msg in messages:
            if "role" not in msg or "content" not in msg:
                continue
            content = msg["content"]
            if isinstance(content, dict):
                content = str(content)
            elif not isinstance(content, str):
                content = str(content)
            clean_msg = {"role": msg["role"], "content": content}
            clean_messages.append(clean_msg)

        # Debug: Log context being sent to text models
        print(f"[{model_name.upper()} CONTEXT] Sending {len(clean_messages)} messages")

        if model_name == "gpt4":
            response = await loop.run_in_executor(
                None,
                lambda: openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=clean_messages,
                    max_tokens=800,
                    temperature=0.7
                )
            )
            return {
                "model": "gpt4",
                "response": response.choices[0].message.content,
                "success": True
            }
        
        elif model_name == "groq":
            response = await loop.run_in_executor(
                None,
                lambda: groq_client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=clean_messages,
                    max_tokens=800,
                    temperature=0.7
                )
            )
            return {
                "model": "groq",
                "response": response.choices[0].message.content,
                "success": True
            }

    except Exception as e:
        return {
            "model": model_name,
            "response": f"Error: {str(e)}",
            "success": False
        }


# ============ MULTIMODAL CHAT ============
# Vision models removed - YOLO handles all detection
# Only text-based chat remains

async def multimodal_chat_async(
    message: str,
    image: Optional[Image.Image],
    conversation_context: list,
    models: list,
    openai_client: OpenAI,
    groq_client: Groq
) -> Dict:
    """
    Multimodal chat - text only (vision removed)
    YOLO handles image detection separately
    """
    tasks = []

    # Only use text models now
    for model in models:
        if model not in ["gemini-vision", "groq-vision"]:
            tasks.append(chat_with_context_async(
                conversation_context,
                model,
                openai_client,
                groq_client
            ))

    if not tasks:
        # No valid models
        return {}

    results = await asyncio.gather(*tasks, return_exceptions=True)

    response_dict = {}
    for i, result in enumerate(results):
        valid_models = [m for m in models if m not in ["gemini-vision", "groq-vision"]]
        if i < len(valid_models):
            if isinstance(result, Exception):
                response_dict[valid_models[i]] = {
                    "response": f"Error: {str(result)}",
                    "success": False
                }
            else:
                response_dict[result.get("model", valid_models[i])] = result["response"]

    return response_dict


# DEPRECATED - Vision functions removed below (kept for reference only)
# All vision analysis now done by YOLO in dental_ai_unified.py

async def _old_vision_with_context_async(
    messages: list,
    image: Image.Image,
    model_name: str,
    openai_client: OpenAI = None,
    groq_client: Groq = None
) -> Dict:
    """
    DEPRECATED: Vision analysis removed
    Use YOLO detection instead
    """
    return {
        "model": model_name,
        "response": "Vision models disabled. Using YOLO for detection.",
        "success": False
    }


# Stub for compatibility
def _old_gemini_vision_handler():
            def sync_gemini_vision():
                model = genai.GenerativeModel(GEMINI_MODEL)
                
                # Get the current user question
                current_question = ""
                conversation_context = ""
                for msg in reversed(messages):
                    if msg['role'] == 'user':
                        if not current_question:
                            current_question = msg['content']
                            break
                
                # Build conversation context
                for msg in messages:
                    if msg['role'] == 'user':
                        conversation_context += f"User: {msg['content']}\n"
                    elif msg['role'] == 'assistant':
                        conversation_context += f"Assistant: {msg['content']}\n"

                # Determine if this is a follow-up or initial analysis
                is_followup = current_question and current_question.lower() not in [
                    "analyze this dental x-ray", 
                    "analyze this x-ray", 
                    "analyze this dental x-ray for wisdom teeth."
                ]
                
                if is_followup:
                    # Follow-up question - natural language response
                    prompt = f"""You are a dental assistant. A dental X-ray image is attached.

The user is asking: "{current_question}"

Previous conversation:
{conversation_context}

Look at the X-ray image and answer the question based on what you see.

[ACADEMIC RESEARCH - EDUCATIONAL SAMPLE]
Answer directly and helpfully. Do NOT use JSON format.
This is for academic research only, not clinical diagnosis."""
                else:
                    # Initial analysis - JSON response with bounding boxes
                    prompt = f"""{conversation_context}

[ACADEMIC RESEARCH - EDUCATIONAL SAMPLE]

Analyze the dental X-ray image. Identify wisdom teeth locations.

For each wisdom tooth found, provide:
1. Position (upper-left, upper-right, lower-left, lower-right)
2. Bounding box coordinates as percentages (x_min, y_min, x_max, y_max)
3. Brief description

Format as JSON:
{{
    "teeth_found": [
        {{
            "position": "lower-right",
            "bbox": [0.6, 0.7, 0.85, 0.95],
            "description": "Impacted wisdom tooth"
        }}
    ],
    "summary": "Brief summary"
}}

If no wisdom teeth visible, return empty teeth_found list.
This is for academic research only."""

                print(f"  ðŸ“¸ Sending image to Gemini Vision")
                try:
                    response = model.generate_content([prompt, image])
                    return response.text
                except Exception as api_error:
                    error_str = str(api_error)
                    if "429" in error_str or "quota" in error_str.lower():
                        print(f"  âš ï¸ Gemini API quota exceeded")
                        return json.dumps({
                            "teeth_found": [],
                            "summary": "âš ï¸ Gemini quota exceeded (~20 requests/day on free tier). Please use Groq Llama Vision."
                        })
                    raise

            response_text = await loop.run_in_executor(None, sync_gemini_vision)
            return {
                "model": "gemini-vision",
                "response": response_text,
                "success": True
            }

        elif model_name == "groq-vision":
            def sync_groq_vision():
                base64_image = encode_image_to_base64(image)
                
                # Get current user message
                user_message = 'Analyze this dental X-ray'
                for msg in reversed(messages):
                    if msg.get('role') == 'user':
                        user_message = msg['content']
                        break
                
                # Determine if follow-up or initial
                is_followup = user_message.lower() not in [
                    "analyze this dental x-ray",
                    "analyze this x-ray",
                    "analyze this dental x-ray for wisdom teeth."
                ]
                
                if is_followup:
                    # Follow-up - natural language
                    vision_prompt = f"""{user_message}

[ACADEMIC RESEARCH PROJECT]

Look at the dental X-ray image and answer the question.
Be specific about what you see. Use natural language, not JSON.
This is for academic research only."""
                else:
                    # Initial analysis - JSON with bounding boxes
                    vision_prompt = f"""{user_message}

[ACADEMIC RESEARCH PROJECT - EDUCATIONAL PURPOSES ONLY]

Analyze this dental X-ray image and identify wisdom teeth locations.

For each wisdom tooth found, provide:
1. Position (upper-left, upper-right, lower-left, lower-right)
2. Bounding box coordinates as percentages (x_min, y_min, x_max, y_max)
3. Brief description of tooth condition

Your response MUST be valid JSON:
{{
    "teeth_found": [
        {{
            "position": "lower-right",
            "bbox": [0.6, 0.7, 0.85, 0.95],
            "description": "Impacted wisdom tooth"
        }}
    ],
    "summary": "Brief overall summary"
}}

If no wisdom teeth visible: {{"teeth_found": [], "summary": "No wisdom teeth detected"}}

Return ONLY the JSON object."""

                # Try each Groq vision model
                for model_id in GROQ_VISION_MODELS:
                    try:
                        print(f"  ðŸ”„ Trying Groq vision: {model_id}")
                        response = groq_client.chat.completions.create(
                            model=model_id,
                            messages=[
                                {
                                    "role": "user",
                                    "content": [
                                        {"type": "text", "text": vision_prompt},
                                        {
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/png;base64,{base64_image}"
                                            }
                                        }
                                    ]
                                }
                            ],
                            max_tokens=1000,
                            temperature=0.3
                        )
                        print(f"  âœ… Success with: {model_id}")
                        return response.choices[0].message.content
                    except Exception as e:
                        print(f"  âš ï¸ {model_id} failed: {str(e)[:80]}")
                        continue
                
                # All failed
                return json.dumps({
                    "teeth_found": [],
                    "summary": "Groq vision models unavailable. Please try again."
                })

            response_text = await loop.run_in_executor(None, sync_groq_vision)
            return {
                "model": "groq-vision",
                "response": response_text,
                "success": True
            }

    except Exception as e:
        import traceback
        print(f"âŒ ERROR in vision_with_context_async: {str(e)}")
        traceback.print_exc()
        
        return {
            "model": model_name,
            "response": f"Error: {str(e)}",
            "success": False
        }


# ============ UNIFIED MULTIMODAL CHAT ============

async def multimodal_chat_async(
    message: str,
    image: Optional[Image.Image],
    conversation_context: list,
    models: list,
    openai_client: OpenAI,
    groq_client: Groq
) -> Dict:
    """
    Unified multimodal chat - handles both text and vision
    """
    tasks = []
    
    for model in models:
        if model in ["gemini-vision", "groq-vision"]:
            tasks.append(vision_with_context_async(
                conversation_context,
                image,
                model,
                openai_client,
                groq_client
            ))
        else:
            tasks.append(chat_with_context_async(
                conversation_context,
                model,
                openai_client,
                groq_client
            ))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    response_dict = {}
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            response_dict[models[i]] = {
                "response": f"Error: {str(result)}",
                "success": False
            }
        else:
            response_dict[result["model"]] = result["response"]
    
    return response_dict