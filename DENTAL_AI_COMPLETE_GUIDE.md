# ü¶∑ Dental AI Platform - Complete Guide

**Version**: 2.4  
**Last Updated**: December 2025  
**Author**: Harsha

---

## üìë Table of Contents

1. [Quick Start](#quick-start)
2. [Project Overview](#project-overview)
3. [System Architecture](#system-architecture)
4. [Installation & Setup](#installation--setup)
5. [Usage Guide](#usage-guide)
6. [YOLO Training Guide](#yolo-training-guide)
7. [API Reference](#api-reference)
8. [Configuration](#configuration)
9. [Troubleshooting](#troubleshooting)
10. [Project Evolution](#project-evolution)
11. [Quick Reference](#quick-reference)

---

## üöÄ Quick Start

### Prerequisites
- Python 3.10+
- 8GB RAM minimum
- GPU recommended (but not required)
- API Keys: OpenAI and Groq

### Installation

```bash
# 1. Navigate to backend
cd backend

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure API keys
cat > .env << EOF
OPEN_AI_API_KEY=your_openai_key
GROQ_AI_API_KEY=your_groq_key
EOF

# 5. Launch application
./run_unified.sh
```

Access at: **http://localhost:7860**

---

## üìñ Project Overview

### What is This Project?

The Dental AI Platform is an intelligent system that combines:
- **YOLOv8 Object Detection** for accurate bounding box detection of wisdom teeth and dental features
- **Multi-Model AI Chat** using GPT-4o-mini, Llama 3.3 70B, and Qwen 3 32B for contextual analysis
- **Unified Chat Interface** that handles both image uploads and text conversations
- **PDF Report Generation** for professional clinical reports
- **Annotation Playground** for testing diagnostic skills

### Key Features

‚úÖ **Accurate Detection**: YOLOv8 model trained specifically on dental X-rays (88% mAP@50)  
‚úÖ **Multi-Model Analysis**: Get insights from 3 different AI models simultaneously  
‚úÖ **Conversation Memory**: Full context awareness for follow-up questions  
‚úÖ **Image Persistence**: Annotated images remain visible throughout conversation  
‚úÖ **Visual Annotations**: Automatic bounding boxes on detected pathologies  
‚úÖ **Professional PDF Reports**: Generate clinical reports with findings and recommendations  
‚úÖ **Real-Time Processing**: <100ms detection + parallel AI inference

### Technology Stack

- **Computer Vision**: YOLOv8 (Ultralytics)
- **AI Models**: 
  - OpenAI GPT-4o-mini
  - Groq Llama 3.3 70B
  - Groq Qwen 3 32B
- **Web Framework**: Gradio 6.x
- **Image Processing**: PIL (Pillow)
- **PDF Generation**: ReportLab
- **Async Processing**: asyncio
- **Dataset Management**: HuggingFace Datasets

---

## üèóÔ∏è System Architecture

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Interface (Gradio)                  ‚îÇ
‚îÇ              dental_ai_unified.py                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   Chat Tab   ‚îÇ  ‚îÇ  Report Tab   ‚îÇ  ‚îÇ  Playground  ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Core Processing Layer                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ multimodal_utils ‚îÇ  ‚îÇ  image_utils     ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  - Routing       ‚îÇ  ‚îÇ  - BBox Drawing  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  - Context       ‚îÇ  ‚îÇ  - Resizing      ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  - Formatting    ‚îÇ  ‚îÇ                  ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              AI & Detection Layer                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ   api_utils.py   ‚îÇ  ‚îÇ  YOLOv8 Model    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  - GPT-4o-mini   ‚îÇ  ‚îÇ  - Detection     ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  - Llama 3.3     ‚îÇ  ‚îÇ  - Filtering     ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  - Qwen 3 32B    ‚îÇ  ‚îÇ                  ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

#### Image Upload Flow

```
User Uploads X-ray
    ‚îÇ
    ‚ñº
YOLO Detection (detect_teeth_yolo)
    ‚îÇ
    ‚îú‚îÄ‚Üí Bounding Box Detection
    ‚îú‚îÄ‚Üí Class Identification
    ‚îú‚îÄ‚Üí Confidence Scoring
    ‚îî‚îÄ‚Üí Spatial Filtering
    ‚îÇ
    ‚ñº
Annotated Image Creation (draw_bounding_boxes)
    ‚îÇ
    ‚îú‚îÄ‚Üí Draw Bounding Boxes
    ‚îú‚îÄ‚Üí Add Labels
    ‚îî‚îÄ‚Üí Color Coding
    ‚îÇ
    ‚ñº
YOLO Results ‚Üí Text Models
    ‚îÇ
    ‚îú‚îÄ‚Üí GPT-4o-mini Analysis
    ‚îú‚îÄ‚Üí Llama 3.3 Analysis
    ‚îî‚îÄ‚Üí Qwen 3 32B Analysis
    ‚îÇ
    ‚ñº
Formatted Response Display
```

#### Follow-up Question Flow

```
User Asks Follow-up Question
    ‚îÇ
    ‚ñº
Route Message (route_message)
    ‚îÇ
    ‚îú‚îÄ‚Üí Check for Recent Image
    ‚îú‚îÄ‚Üí Check Message Context
    ‚îî‚îÄ‚Üí Determine Models
    ‚îÇ
    ‚ñº
Build Conversation Context (build_conversation_context)
    ‚îÇ
    ‚îú‚îÄ‚Üí Retrieve Last N Turns
    ‚îú‚îÄ‚Üí Include YOLO Detection Results
    ‚îú‚îÄ‚Üí Include System Prompt
    ‚îî‚îÄ‚Üí Format for API
    ‚îÇ
    ‚ñº
Parallel API Calls (multimodal_chat_async)
    ‚îÇ
    ‚îú‚îÄ‚Üí GPT-4o-mini (async)
    ‚îú‚îÄ‚Üí Llama 3.3 (async)
    ‚îî‚îÄ‚Üí Qwen 3 32B (async)
    ‚îÇ
    ‚ñº
Format & Display Response
```

### Component Details

#### 1. `dental_ai_unified.py` (Main Application)

**Purpose**: Gradio UI orchestration and user interaction handling

**Key Functions**:
- `process_chat_message()`: Main message processing pipeline
- `clear_conversation()`: Reset conversation state
- `generate_report_pdf()`: PDF report generation
- UI event handlers for chat, report, and playground tabs

**State Management**:
- `conversation_state`: Full conversation history with metadata
- `stored_annotated_images`: Persistent annotated image storage
- `selected_model`: Currently selected AI model for display

#### 2. `api_utils.py` (API & Detection Layer)

**Purpose**: Handles all AI model interactions and YOLO detection

**Key Functions**:
- `detect_teeth_yolo()`: YOLO object detection with filtering
- `chat_with_context_async()`: Context-aware chat with text models
- `multimodal_chat_async()`: Parallel multi-model chat execution
- `get_yolo_model()`: Lazy loading of YOLO model

**Detection Pipeline**:
1. Load YOLO model (lazy initialization)
2. Run inference on image
3. Apply class-specific confidence thresholds
4. Apply spatial filtering for impacted teeth
5. Convert to normalized coordinates
6. Return structured detection results

#### 3. `multimodal_utils.py` (Routing & Formatting)

**Purpose**: Message routing, context building, and response formatting

**Key Functions**:
- `route_message()`: Determine which models to use
- `build_conversation_context()`: Build API-ready message context
- `format_multi_model_response()`: Format text responses
- `format_vision_response()`: Format YOLO + text model responses

**Routing Logic**:
- **Image Upload**: `["gpt4", "groq", "qwen"]` (YOLO + text analysis)
- **Follow-up with Image**: `["gpt4", "groq", "qwen"]` (text with context)
- **Text Only**: `["gpt4", "groq", "qwen"]` (general chat)

#### 4. `image_utils.py` (Image Processing)

**Purpose**: Image manipulation and annotation

**Key Functions**:
- `draw_bounding_boxes()`: Draw annotated bounding boxes on images
- `resize_image_for_chat()`: Resize images for chat display

**Features**:
- Color-coded bounding boxes by class
- Confidence score display
- Position-based color mapping
- Thick outlines for visibility
- Dynamic label positioning to prevent cutoff

#### 5. `report_generator.py` (PDF Report Generation)

**Purpose**: Generate professional PDF clinical reports

**Key Functions**:
- `generate_pdf_report()`: Main PDF generation function
- `markdown_to_html()`: Convert markdown to HTML for ReportLab
- `calculate_severity()`: Determine condition severity
- `calculate_priority()`: Determine treatment priority
- `get_fdi_notation()`: Convert position to FDI tooth notation

**Report Sections**:
- Professional header with clinic name and Report ID
- Original and annotated X-ray images
- Risk Assessment Score (High/Medium/Low)
- Dental Health Summary
- Detailed Findings Table
- Patient-Friendly Explanation
- Treatment Recommendations
- Next Steps Checklist
- Professional footer with disclaimer

#### 6. `dataset_utils.py` (Dataset Management)

**Purpose**: Dataset integration with automatic filtering

**Key Class**: `TeethDatasetManager`

**Methods**:
- `load_metadata()`: Load dataset metadata (checks local first, then HuggingFace)
- `get_sample()`: Get specific sample by index (automatically skips binary/mask images)
- `get_next_sample()`: Get next sample in sequence
- `get_previous_sample()`: Get previous sample in sequence
- `scan_dataset_quality()`: Analyze dataset quality (valid X-rays vs binary masks)
- `get_random_sample()`: Get random valid X-ray

**Features**:
- **Automatic Binary Filtering**: Skips binary/mask images (only 2 unique values)
- **Local Dataset Support**: Automatically detects and uses local YOLO format datasets
- **Quality Validation**: Validates images have ‚â•10 unique values and reasonable brightness (5-250)
- **Smart Retry**: Tries up to 50 different indices to find valid X-rays
- **Lazy Loading**: Only loads images when needed
- **Caching**: LRU cache for performance
- **Grayscale Preservation**: Proper grayscale-to-RGB conversion preserving full range (0-255)

**Dataset Sources**:
1. **Local YOLO Dataset** (if available): Checks `Dental-X-ray-1/train/images` or `datasets/Dental-X-ray-1/train/images`
2. **HuggingFace** (fallback): `RayanAi/Main_teeth_dataset` (with automatic filtering)

#### 7. `annotation_playground.py` (Annotation Playground)

**Purpose**: Interactive playground for testing diagnostic skills

**Key Functions**:
- `load_uploaded_image()`: Load user-uploaded X-ray image
- `load_random_playground_image()`: Load random valid X-ray from dataset (skips binary images)
- `handle_image_click()`: Handle user clicks on image
- `clear_playground()`: Clear user annotations
- `compare_with_ai()`: Compare user clicks with AI detections
- `save_image_for_gradio()`: Save PIL images to temp files for Gradio display

**Features**:
- **Filepath-Based Display**: Uses filepaths instead of PIL objects (fixes image corruption issues)
- **Image Upload Support**: Users can upload their own X-ray images
- **Click-Based Annotation**: Simple click-based marking (no external libraries)
- **Score Calculation**: Precision, recall, F1-like score
- **Statistics Tracking**: Attempts, average score
- **Side-by-Side Comparison**: Shows user marks vs AI detections
- **Grayscale Preservation**: Proper conversion preserving full grayscale range

---

## üì¶ Installation & Setup

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (recommended for YOLO, but CPU works)
- API Keys:
  - OpenAI API key (for GPT-4o-mini)
  - Groq API key (for Llama 3.3 and Qwen)

### Step-by-Step Setup

#### 1. Clone/Download Project

```bash
cd /path/to/project
```

#### 2. Create Virtual Environment

```bash
cd backend
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**Key Dependencies**:
- `gradio>=6.0.0`: Web UI framework
- `ultralytics>=8.0.0`: YOLOv8 implementation
- `openai>=1.0.0`: OpenAI API client
- `groq>=0.11.0`: Groq API client
- `pillow>=10.0.0`: Image processing
- `numpy>=1.24.0`: Numerical operations
- `datasets>=3.0.0`: HuggingFace datasets
- `python-dotenv>=1.0.0`: Environment variable management
- `reportlab>=4.0.0`: PDF generation

#### 4. Configure API Keys

Create `.env` file in `backend/` directory:

```env
OPEN_AI_API_KEY=your_openai_api_key_here
GROQ_AI_API_KEY=your_groq_api_key_here
```

#### 5. Download YOLO Model (Optional)

The system will automatically download `yolov8n.pt` if needed, but you can pre-download:

```bash
python -c "from ultralytics import YOLO; YOLO('yolov8n.pt')"
```

#### 6. Place Trained Model (If Available)

If you have a trained model, place it in:
```
backend/models/dental_impacted.pt
```

The system will automatically use it if found.

#### 7. Run Application

```bash
./run_unified.sh
# Or directly:
python dental_ai_unified.py
```

Access at: `http://localhost:7860`

---

## üìñ Usage Guide

### Basic Usage

#### 1. Starting the Application

```bash
cd backend
./run_unified.sh
```

Or directly:
```bash
python dental_ai_unified.py
```

#### 2. Using the Chat Interface

**Upload X-ray**:
1. Click "Upload X-Ray Image" button
2. Select X-ray image file
3. Optionally type a message
4. Click "Send"

**Ask Follow-up Questions**:
1. Type your question in the text box
2. Click "Send"
3. The system remembers the previous X-ray and YOLO detection results

**Select AI Model Response**:
- Click on "GPT-4o-mini", "Llama 3.3 70B", or "Qwen 3 32B" buttons
- Only the selected model's response will be displayed
- Previous messages update to show only the selected model's responses

**Clear Conversation**:
- Click "üóëÔ∏è Clear Chat" button

#### 3. Generating PDF Reports

1. Upload an X-ray and run analysis
2. Go to "üìÑ PDF Report" tab
3. Click "üìÑ Generate PDF Report"
4. Download the generated PDF report

**Report Includes**:
- Patient information header
- Original and annotated X-ray images
- Risk Assessment Score
- Detailed Findings Table
- AI Analysis Summary
- Treatment Recommendations
- Next Steps Checklist
- Professional disclaimer

#### 4. Using Annotation Playground

1. Go to "üéØ Annotation Playground" tab
2. **Upload an X-ray image** using the uploader (recommended) OR click "üé≤ Random X-Ray" to load from dataset
3. Click on areas where you see dental issues (wisdom teeth, cavities, etc.)
4. Click "‚úÖ Compare with AI" to see how your marks compare with AI detection
5. View your score and feedback

**Note**: The system automatically filters out binary/mask images from the dataset and only shows real X-rays with full grayscale range.

### Advanced Usage

#### Dataset Quality Scanning

Check dataset quality to see how many images are valid X-rays vs binary masks:

```bash
cd backend
source venv/bin/activate
python scan_dataset_quality.py
```

This will show:
- Number of valid X-rays (with full grayscale range)
- Number of binary/mask images (filtered out)
- Percentage of usable images

#### Using Local Dataset

The system automatically detects local YOLO format datasets. Place your dataset at:
- `backend/Dental-X-ray-1/train/images/` OR
- `backend/datasets/Dental-X-ray-1/train/images/`

The system will use the local dataset instead of HuggingFace if found.

#### Custom Confidence Thresholds

Modify `api_utils.py`:
```python
CLASS_CONFIDENCE_THRESHOLDS = {
    "Impacted Tooth": 0.25,  # Adjust this value
    "Cavity": 0.30,          # Adjust this value
    # ...
}
```

#### Custom Model Path

Update in `api_utils.py`:
```python
YOLO_IMPACTED_MODEL_PATH = "path/to/your/model.pt"
```

---

## üéì YOLO Training Guide

### Training Overview

A YOLOv8 object detection model specifically trained to detect dental features in X-ray images, with a focus on wisdom teeth detection.

### Training Results Summary

**Final Performance** (from actual training run):
- **mAP50**: ~0.90 (90% accuracy at IoU=0.5)
- **mAP50-95**: ~0.60 (60% accuracy across IoU thresholds)
- **Precision**: ~0.85
- **Recall**: ~0.80
- **Training Time**: ~100 minutes on GPU
- **Best Model**: Saved at epoch ~50-60 (early stopping)

### Dataset Preparation

#### Dataset Source

**Roboflow Dental X-ray Dataset**
- Format: YOLO format (images + YOLO annotation files)
- Structure: Train/Valid/Test splits
- Total Images: 1,075
  - Training: 753 images
  - Validation: 215 images
  - Test: 107 images

#### Dataset Structure

```
Dental-X-ray-1/
‚îú‚îÄ‚îÄ data.yaml                    # Dataset configuration
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_001.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_002.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îÇ       ‚îú‚îÄ‚îÄ image_001.txt        # YOLO format: class_id x y w h
‚îÇ       ‚îú‚îÄ‚îÄ image_002.txt
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ valid/
‚îÇ   ‚îú‚îÄ‚îÄ images/                  # 215 validation images
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ images/                  # 107 test images
    ‚îî‚îÄ‚îÄ labels/
```

#### data.yaml Format

```yaml
path: /absolute/path/to/Dental-X-ray-1
train: train/images
val: valid/images
test: test/images

nc: 6  # Number of classes
names:
  0: Impacted
  1: Cavity
  2: Deep Caries
  3: Filling
  4: Implant
  5: Crown
```

### Training Configuration

#### Complete Training Parameters

**Model Configuration**:
- **Base Model**: `yolov8n.pt` (YOLOv8 Nano)
- **Task**: Object Detection
- **Image Size**: 640x640 pixels
- **Batch Size**: 16
- **Epochs**: 100
- **Patience**: 20 (early stopping)

**Optimization**:
- **Optimizer**: Auto (AdamW)
- **Initial Learning Rate (lr0)**: 0.01
- **Final Learning Rate (lrf)**: 0.01
- **Momentum**: 0.937
- **Weight Decay**: 0.0005
- **Warmup Epochs**: 3.0

**Data Augmentation**:
- **Mosaic**: 1.0 (100% probability)
- **Mixup**: 0.0 (disabled)
- **Flip LR**: 0.5 (50% horizontal flip)
- **HSV-H**: 0.015 (hue variation)
- **HSV-S**: 0.7 (saturation variation)
- **HSV-V**: 0.4 (value/brightness variation)
- **Translation**: 0.1 (10% translation)
- **Scale**: 0.5 (50% scaling)

### Training Execution

#### Step 1: Environment Setup

```bash
# Activate virtual environment
cd backend
source venv/bin/activate

# Verify dependencies
pip list | grep ultralytics
# Should show: ultralytics >= 8.0.0
```

#### Step 2: Verify Dataset

```bash
# Check dataset exists
ls -la Dental-X-ray-1/data.yaml

# Inspect dataset
python inspect_dataset.py --data Dental-X-ray-1/data.yaml
```

#### Step 3: Start Training

**Option A: Using Training Script (Recommended)**

```bash
python train_yolo_dental.py \
    --model n \
    --epochs 100 \
    --imgsz 640 \
    --batch 16 \
    --data Dental-X-ray-1/data.yaml \
    --project dental_detection
```

**Option B: Direct Python**

```python
from ultralytics import YOLO

model = YOLO('yolov8n.pt')
results = model.train(
    data='Dental-X-ray-1/data.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    project='dental_detection',
    name='dental_wisdom_detection'
)
```

#### Step 4: Training Monitoring

**Real-time Output**:
```
üöÄ Starting YOLO training with yolov8n.pt
   Epochs: 100, Image Size: 640, Batch: 16
üìä Using dataset: Dental-X-ray-1/data.yaml

Train: 47/47 [00:15<00:00, 3.12it/s, box_loss=1.97, cls_loss=3.26, dfl_loss=1.54]
Val:   7/7 [00:02<00:00, 3.45it/s, box_loss=1.54, cls_loss=3.30, dfl_loss=1.34]

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss   Instances       Size
        1/100      2.1G      1.973      3.264      1.542         47        640
                 Class     Images  Instances          P          R      mAP50   mAP50-95
                   all        215        215      0.047      0.364      0.123      0.058
```

**Training Progress** (from actual training):

| Epoch | mAP50 | mAP50-95 | Precision | Recall |
|-------|-------|----------|-----------|--------|
| 1     | 0.123 | 0.058    | 0.047     | 0.364  |
| 10    | 0.734 | 0.452    | 0.669     | 0.710  |
| 20    | 0.794 | 0.510    | 0.766     | 0.707  |
| 50    | ~0.85 | ~0.55    | ~0.80     | ~0.75  |
| 100   | ~0.90 | ~0.60    | ~0.85     | ~0.80  |

#### Step 5: Model Deployment

1. **Copy Best Model**:
   ```bash
   cp runs/detect/dental_wisdom_detection/weights/best.pt \
      backend/models/dental_impacted.pt
   ```

2. **Test Model**:
   - Run the application
   - Upload a test X-ray
   - Verify detections are accurate

### Training Tips & Best Practices

#### Model Selection

- **YOLOv8n (Nano)**: Fastest, smallest, good for development
- **YOLOv8s (Small)**: Better accuracy, still fast
- **YOLOv8m (Medium)**: Best balance
- **YOLOv8l (Large)**: High accuracy, slower
- **YOLOv8x (XLarge)**: Highest accuracy, slowest

#### Hyperparameter Tuning

**Learning Rate**:
- Start with 0.01
- Reduce if loss doesn't decrease
- Increase if loss decreases too slowly

**Batch Size**:
- Larger batch = more stable training, more memory
- Adjust based on GPU memory
- Common: 8, 16, 32, 64

**Image Size**:
- Larger = better accuracy, slower training
- Common: 640, 800, 1280
- Must match inference size

### Troubleshooting Training

**Low mAP**:
- Check label quality
- Increase training epochs
- Try larger model
- Increase image size

**Overfitting**:
- Increase data augmentation
- Add dropout
- Reduce model size
- Early stopping

**Memory Issues**:
- Reduce batch size
- Reduce image size
- Reduce workers
- Disable cache

---

## üìö API Reference

### YOLO Detection API

#### `detect_teeth_yolo(image, conf_threshold=0.35, iou_threshold=0.4)`

Detect dental features in X-ray image.

**Parameters**:
- `image` (PIL.Image): Input X-ray image
- `conf_threshold` (float): Base confidence threshold (default: 0.35)
- `iou_threshold` (float): NMS IoU threshold (default: 0.4)

**Returns**:
```python
{
    "success": bool,
    "model": str,
    "teeth_found": [
        {
            "position": str,  # "upper-left", "upper-right", etc.
            "bbox": [float, float, float, float],  # [x_min, y_min, x_max, y_max] normalized
            "confidence": float,
            "class_name": str,
            "description": str
        }
    ],
    "summary": str
}
```

**Class-Specific Thresholds**:
- Impacted: 0.25
- Cavity/Caries: 0.30
- Fillings/Implants: 0.30
- Default: 0.35

**Spatial Filtering**:
- Impacted teeth filtered if in middle of jaw (x between 0.30 and 0.70)
- Only accepts impacted teeth at edges (x < 0.30 or x > 0.70)

### Text Model API

#### `chat_with_context_async(messages, model_name, openai_client, groq_client)`

Chat with context-aware text models.

**Parameters**:
- `messages` (list): List of message dicts with 'role' and 'content'
- `model_name` (str): "gpt4", "groq", or "qwen"
- `openai_client` (OpenAI): OpenAI client instance
- `groq_client` (Groq): Groq client instance

**Returns**:
```python
{
    "model": str,
    "response": str,
    "success": bool
}
```

**Supported Models**:
- `"gpt4"`: GPT-4o-mini (OpenAI)
- `"groq"`: Llama 3.3 70B (Groq)
- `"qwen"`: Qwen 3 32B (Groq)

#### `multimodal_chat_async(message, image, conversation_context, models, openai_client, groq_client)`

Parallel multi-model chat execution.

**Parameters**:
- `message` (str): User message
- `image` (PIL.Image, optional): Image (unused, kept for compatibility)
- `conversation_context` (list): Full conversation context
- `models` (list): List of model names to use
- `openai_client` (OpenAI): OpenAI client
- `groq_client` (Groq): Groq client

**Returns**:
```python
{
    "gpt4": str,      # Response from GPT-4o-mini
    "groq": str,      # Response from Llama 3.3
    "qwen": str       # Response from Qwen
}
```

### Image Processing API

#### `draw_bounding_boxes(image, detections, show_confidence=True)`

Draw bounding boxes on image.

**Parameters**:
- `image` (PIL.Image): Input image
- `detections` (list): List of detection dicts
- `show_confidence` (bool): Show confidence scores (default: True)

**Returns**: PIL.Image with bounding boxes drawn

**Color Mapping**:
- Impacted: Red (#FF6B6B)
- Caries: Orange (#FF9F40)
- Deep Caries: Dark Red (#FF4444)
- Periapical Lesion: Purple (#9F40FF)
- Crown: Teal (#4ECDC4)
- Filling: Mint (#95E1D3)
- Implant: Yellow (#FFE66D)

---

## ‚öôÔ∏è Configuration

### Detection Configuration

#### Confidence Thresholds

Edit `backend/api_utils.py`:

```python
CLASS_CONFIDENCE_THRESHOLDS = {
    "Impacted Tooth": 0.25,  # Lower = more sensitive
    "Cavity": 0.30,
    "Deep Caries": 0.30,
    "Filling": 0.30,
    "Implant": 0.30,
    "Crown": 0.30
}
```

#### Spatial Filtering

For impacted teeth, adjust in `api_utils.py`:

```python
# Impacted teeth must be at jaw edges
# x < 0.30 (left edge) or x > 0.70 (right edge)
IMPACTED_X_THRESHOLD_LEFT = 0.30
IMPACTED_X_THRESHOLD_RIGHT = 0.70
```

### Model Configuration

#### YOLO Model Path

```python
# In api_utils.py
YOLO_IMPACTED_MODEL_PATH = "models/dental_impacted.pt"
```

#### AI Model Selection

Edit `backend/multimodal_utils.py`:

```python
# Available models: "gpt4", "groq", "qwen"
models = ["gpt4", "groq", "qwen"]
```

### PDF Report Configuration

Edit `backend/report_generator.py`:

```python
# Clinic information
CLINIC_NAME = "Your Dental Clinic"
CLINIC_ADDRESS = "123 Main St, City, State"
```

---

## üêõ Troubleshooting

### Common Issues

**YOLO Model Not Found**:
- Check `models/dental_impacted.pt` exists
- System will use base YOLOv8n as fallback
- Download base model: `python -c "from ultralytics import YOLO; YOLO('yolov8n.pt')"`

**API Key Errors**:
- Check `.env` file exists
- Verify API keys are correct
- Check API key permissions

**Memory Issues**:
- Reduce batch size in training
- Use smaller YOLO model (nano instead of large)
- Reduce image size

**Slow Performance**:
- Use GPU for YOLO (automatic if available)
- Reduce number of models (modify routing)
- Cache YOLO model (already implemented)

**No Detections**:
- Check confidence thresholds
- Verify model is trained on similar data
- Check image quality and format

**Port Already in Use**:
```bash
# Find process using port 7860
lsof -ti:7860

# Kill the process
kill <PID>

# Or change port in dental_ai_unified.py
demo.launch(server_port=7861)
```

**Image Dragging in Annotation Playground**:
- This is a known Firefox issue
- The playground includes CSS/JavaScript to prevent dragging
- If issues persist, try Chrome or Edge

**PDF Report Errors**:
- Ensure `original_image` is not None
- Check that YOLO detections are present
- Verify AI analysis responses are not empty

**Binary/Black Images in Annotation Playground**:
- The HuggingFace dataset contains binary segmentation masks mixed with real X-rays
- The system automatically filters these out (only shows images with ‚â•10 unique values)
- If you see binary images, check console logs for `[SKIP]` messages
- **Solution**: Upload your own X-ray images using the uploader (recommended)
- **Alternative**: Use local dataset by placing images at `Dental-X-ray-1/train/images/`

**Dataset Quality Issues**:
- Run quality scan: `python scan_dataset_quality.py`
- If <50% valid images, consider using a different dataset
- System automatically skips binary/mask images (tries up to 50 indices to find valid X-ray)

**Image Corruption/Static Noise**:
- Fixed by using filepath-based display instead of PIL objects
- Images are saved to temp files before display
- Grayscale images properly converted to RGB preserving full range (0-255)

---

## üìà Project Evolution

### Development Timeline

**Phase 1: Vision Models Only** (Week 1)
- Used GPT-4 Vision and Gemini Vision
- Problem: Coordinate hallucination
- Result: Inaccurate bounding boxes

**Phase 2: The Hallucination Problem** (Week 1-2)
- Discovered vision models made up coordinates
- Attempted prompt engineering (failed)
- Realized need for detection model

**Phase 3: YOLO Integration** (Week 2)
- Integrated YOLO for detection
- Removed vision models
- Used text models to analyze YOLO results
- Result: Accurate bounding boxes

**Phase 4: Multi-Model Text Analysis** (Week 2-3)
- Added Llama 3.3 70B (Groq)
- Added Qwen 3 32B (Groq) - updated from Qwen 2.5
- Parallel execution for speed
- Result: Multi-model consensus

**Phase 7: Dataset Quality & Image Display Fixes** (Week 4)
- Discovered HuggingFace dataset contains binary segmentation masks
- Implemented automatic filtering (skips images with <10 unique values)
- Fixed image corruption by switching to filepath-based Gradio display
- Fixed grayscale-to-RGB conversion preserving full range (0-255)
- Added local dataset support (automatic detection)
- Added quality scanning functionality
- Result: Only real X-rays displayed, no binary masks

**Phase 5: Custom YOLO Training** (Week 3)
- Trained custom model on dental X-rays
- Dataset: 1,075 annotated images from Roboflow
- Result: 88% mAP@50 accuracy

**Phase 6: Detection Refinement** (Week 3)
- Class-specific confidence thresholds
- Spatial filtering for impacted teeth
- Adjusted NMS parameters
- Result: Production-ready system

### Key Learnings

1. **Vision Models ‚â† Detection Models**: Vision models are for understanding, not localization
2. **Training Your Own Model > Pre-trained**: Domain-specific data >> Generic data
3. **Multi-Model Consensus is Powerful**: Different models catch different things
4. **Filtering is as Important as Detection**: Raw model output needs refinement
5. **Iterative Refinement Based on User Feedback**: Start with baseline, adjust incrementally
6. **Dataset Quality Matters**: Public datasets may contain mixed content (real images + masks)
7. **Filepath > PIL Objects for Gradio**: Filepath-based display prevents image corruption issues
8. **Grayscale Conversion Must Preserve Range**: NumPy stacking preserves full range better than PIL convert

---

## üìã Quick Reference

### Quick Start Commands

```bash
# Run application
cd backend && ./run_unified.sh

# Activate virtual environment
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Train model
python train_yolo_dental.py --epochs 100 --batch 16
```

### File Structure

```
dk_project_2/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ dental_ai_unified.py      # Main application
‚îÇ   ‚îú‚îÄ‚îÄ api_utils.py               # YOLO + API calls
‚îÇ   ‚îú‚îÄ‚îÄ multimodal_utils.py        # Routing & formatting
‚îÇ   ‚îú‚îÄ‚îÄ image_utils.py             # Image processing
‚îÇ   ‚îú‚îÄ‚îÄ dataset_utils.py           # Dataset manager
‚îÇ   ‚îú‚îÄ‚îÄ report_generator.py        # PDF report generation
‚îÇ   ‚îú‚îÄ‚îÄ annotation_playground.py   # Annotation playground
‚îÇ   ‚îú‚îÄ‚îÄ train_yolo_dental.py       # Training script
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dental_impacted.pt     # Trained YOLO model
‚îÇ   ‚îî‚îÄ‚îÄ run_unified.sh             # Launcher
‚îú‚îÄ‚îÄ README.md                       # Project README
‚îî‚îÄ‚îÄ DENTAL_AI_COMPLETE_GUIDE.md    # This file
```

### Key Functions

**YOLO Detection**:
```python
from api_utils import detect_teeth_yolo
result = detect_teeth_yolo(image, conf_threshold=0.35)
```

**Multi-Model Chat**:
```python
from api_utils import multimodal_chat_async
responses = await multimodal_chat_async(
    message, image, context, models, clients
)
```

**Image Annotation**:
```python
from image_utils import draw_bounding_boxes
annotated = draw_bounding_boxes(image, detections)
```

**Dataset Quality Scan**:
```python
from dataset_utils import get_teeth_dataset_manager
manager = get_teeth_dataset_manager()
result = manager.scan_dataset_quality(num_samples=100)
print(result["message"])
```

**Get Valid X-Ray Sample** (automatically skips binary images):
```python
from dataset_utils import get_teeth_dataset_manager
manager = get_teeth_dataset_manager()
sample = manager.get_sample(index)  # Automatically filters binary/mask images
if sample["success"]:
    image = sample["image"]  # Guaranteed to be valid X-ray
```

### Model Classes

- **Impacted**: Impacted wisdom teeth
- **Cavity**: Dental cavities
- **Deep Caries**: Deep tooth decay
- **Filling**: Dental fillings
- **Implant**: Dental implants
- **Crown**: Dental crowns

### Configuration Quick Reference

| Setting | Location | Default |
|---------|----------|---------|
| Impacted Threshold | `api_utils.py` | 0.25 |
| Cavity Threshold | `api_utils.py` | 0.30 |
| YOLO Model Path | `api_utils.py` | `models/dental_impacted.pt` |
| Max Tokens | `api_utils.py` | 2000 |
| Server Port | `dental_ai_unified.py` | 7860 |

---

## ‚ö†Ô∏è Disclaimer

**IMPORTANT DISCLAIMER**: This system is for educational and research purposes only. It is NOT intended for clinical diagnosis or medical decision-making. Always consult licensed dental professionals for actual dental care.

---

## üìú License

- YOLOv8: AGPL-3.0 (Ultralytics)
- Dataset: CC BY 4.0 (Roboflow)
- AI APIs: See provider terms

---

## üë®‚Äçüíª Author

**Harsha**  
Version: 2.4  
December 2025

---

## üôè Acknowledgments

- Ultralytics (YOLOv8)
- Roboflow (Dataset)
- Groq (Llama & Qwen inference)
- OpenAI (GPT-4o-mini)
- HuggingFace (Dataset hosting)

---

**Built with ‚ù§Ô∏è for dental AI**

*Last Updated: December 2025*

